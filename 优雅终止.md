# 优雅终止

> 本文有也有视频教程版: https://www.bilibili.com/video/BV1fu411m73C

## 概述

Pod 销毁时，会停止容器内的进程，通常在停止的过程中我们需要执行一些善后逻辑，比如等待存量请求处理完以避免连接中断，或通知相关依赖进行清理等，从而实现优雅终止目的。本文介绍在 Kubernetes 场景下，实现容器优雅终止的最佳实践。

## 容器终止流程

我们先了解下容器在 Kubernetes 环境中的终止流程:

1. Pod 被删除，状态置为 `Terminating`。
2. kube-proxy 更新转发规则，将 Pod 从 service 的 endpoint 列表中摘除掉，新的流量不再转发到该 Pod。
3. xxxxxxxxxx # socket buffer优化net.ipv4.tcp_wmem = 4096        16384   4194304net.ipv4.tcp_rmem = 4096        87380   6291456net.ipv4.tcp_mem = 381462       508616  762924net.core.rmem_default = 8388608net.core.rmem_max = 26214400 # 读写 buffer 调到 25M 避免大流量时导致 buffer 满而丢包 "netstat -s" 可以看到 receive buffer errors 或 send buffer errorsnet.core.wmem_max = 26214400 # timewait相关优化net.ipv4.tcp_max_tw_buckets = 131072 # 这个优化意义不大net.ipv4.tcp_timestamps = 1  # 通常默认本身是开启的net.ipv4.tcp_tw_reuse = 1 # 仅对客户端有效果，对于高并发客户端，可以复用TIME_WAIT连接端口，避免源端口耗尽建连失败net.ipv4.ip_local_port_range="1024 65535" # 对于高并发客户端，加大源端口范围，避免源端口耗尽建连失败（确保容器内不会监听源端口范围的端口)net.ipv4.tcp_fin_timeout=30 # 缩短TIME_WAIT时间,加速端口回收 # 握手队列相关优化net.ipv4.tcp_max_syn_backlog = 10240 # 没有启用syncookies的情况下，syn queue(半连接队列)大小除了受somaxconn限制外，也受这个参数的限制，默认1024，优化到8096，避免在高并发场景下丢包net.core.somaxconn = 65535 # 表示socket监听(listen)的backlog上限，也就是就是socket的监听队列(accept queue)，当一个tcp连接尚未被处理或建立时(半连接状态)，会保存在这个监听队列，默认为 128，在高并发场景下偏小，优化到 32768。参考 https://imroc.io/posts/kubernetes-overflow-and-drop/net.ipv4.tcp_syncookies = 1​# fd优化fs.file-max=1048576 # 提升文件句柄上限，像 nginx 这种代理，每个连接实际分别会对 downstream 和 upstream 占用一个句柄，连接量大的情况下句柄消耗就大。fs.inotify.max_user_instances="8192" # 表示同一用户同时最大可以拥有的 inotify 实例 (每个实例可以有很多 watch)fs.inotify.max_user_watches="524288" # 表示同一用户同时可以添加的watch数目（watch一般是针对目录，决定了同时同一用户可以监控的目录数量) 默认值 8192 在容器场景下偏小bash
4. kubelet 对 Pod 中各个 container 发送 `SIGTERM` 信号以通知容器进程开始优雅停止。
5. 等待容器进程完全停止，如果在 `terminationGracePeriodSeconds` 内 (默认 30s) 还未完全停止，就发送 `SIGKILL` 信号强制杀死进程。
6. 所有容器进程终止，清理 Pod 资源。

## 业务代码处理 SIGTERM 信号

要实现优雅终止，务必在业务代码里面处理下 `SIGTERM` 信号，主要逻辑是不接受增量连接，继续处理存量连接，所有连接断完才退出，参考 [处理 SIGTERM 代码示例](https://imroc.cc/k8s/ref/code-example-of-handle-sigterm/) 。

## 别让 shell 导致收不到 SIGTERM 信号

如果容器启动入口使用了脚本 (如 `CMD ["/start.sh"]`)，业务进程就成了 shell 的子进程，在 Pod 停止时业务进程可能收不到 `SIGTERM` 信号，因为 shell 不会自动传递信号给子进程。更详细解释请参考 [为什么我的容器收不到 SIGTERM 信号 ?](https://imroc.cc/k8s/faq/why-cannot-receive-sigterm/)

如何解决？请参考 [实用技巧: 在 SHELL 中传递信号](https://imroc.cc/k8s/trick/propagating-signals-in-shell/) 。

## 合理使用 preStop Hook

若你的业务代码中没有处理 `SIGTERM` 信号，或者你无法控制使用的第三方库或系统来增加优雅终止的逻辑，也可以尝试为 Pod 配置下 preStop，在这里面实现优雅终止的逻辑，示例:

```yaml
        lifecycle:
          preStop:
            exec:
              command:
              - /clean.sh
```

> 参考 [Kubernetes API 文档](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#lifecycle-1)

在某些极端情况下，Pod 被删除的一小段时间内，仍然可能有新连接被转发过来，因为 kubelet 与 kube-proxy 同时 watch 到 pod 被删除，kubelet 有可能在 kube-proxy 同步完规则前就已经停止容器了，这时可能导致一些新的连接被转发到正在删除的 Pod，而通常情况下，当应用受到 `SIGTERM` 后都不再接受新连接，只保持存量连接继续处理，所以就可能导致 Pod 删除的瞬间部分请求失败。

这种情况下，我们也可以利用 preStop 先 sleep 一小下，等待 kube-proxy 完成规则同步再开始停止容器内进程:

```yaml
        lifecycle:
          preStop:
            exec:
              command:
              - sleep
              - 5s
```

## 调整优雅时长

如果需要的优雅终止时间比较长 (preStop + 业务进程停止可能超过 30s)，可根据实际情况自定义 `terminationGracePeriodSeconds`，避免过早的被 `SIGKILL` 杀死，示例:

![img](D:/desktop/md/images/1-16599421006584.png)

```
kind: Deployment
apiVersion: apps/v1
metadata:
  name: openapi-web
  namespace: open-iot-ink-api
  labels:
    app: openapi-web
  annotations:
    deployment.kubernetes.io/revision: '25'
spec:
  replicas: 1
  selector:
    matchLabels:
      app: openapi-web
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: openapi-web
      annotations:
        kubesphere.io/restartedAt: '2022-08-09T03:11:51.409Z'
        logging.kubesphere.io/logsidecar-config: '{}'
    spec:
      volumes:
        - name: volume-yc9k93
          configMap:
            name: openapi
            defaultMode: 420
      containers:
        - name: container-cq9y5p
          image: >-
            harbor.iot.store:8085/turing-kubesphere/turing-openapi-web:SNAPSHOT-37
          ports:
            - name: http-80
              containerPort: 80
              protocol: TCP
          resources: {}
          volumeMounts:
            - name: volume-yc9k93
              readOnly: true
              mountPath: /etc/nginx/conf.d
          readinessProbe:
            httpGet:
              path: /
              port: 80
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /
              port: 80
              scheme: HTTP
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 30
          lifecycle:
            preStop:
              exec:
                command:
                  - sleep
                  - 5s
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      serviceAccountName: default
      serviceAccount: default
      securityContext: {}
      imagePullSecrets:
        - name: harbor-repository
      affinity: {}
      schedulerName: default-scheduler
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
```

